---
title: "Data Science 1 HW"
author: "John Joyce"
date: "2/19/2021"
output: html_document
  theme: "cosmo"
  toc: true
  toc_float: true
    collapsed: false
    smooth_scroll: false
  code_folding: hide
  code_download: "yes"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(datasets)
library(MASS)
library(ISLR)
library(caret)
library(skimr)
library(SmartEDA) # summary statistics relative to user-defined target variable
library(kableExtra)
library(factoextra) # find + viz optimal number of clusters for kmeans clustering
library(NbClust)

options(digits = 3)
```
# 1: Penalized Models and PCA

## a.  
Below is a quick summary of the data:  
```{r Q1 a1, echo =  FALSE, warning = FALSE, message = FALSE}

#  load property value data
data <- readRDS(url('http://www.jaredlander.com/data/manhattan_Train.rds')) %>%
  mutate(logTotalValue = log(TotalValue)) %>%
  drop_na()

# summary EDA as scrollable HTML table
skim(data) %>% 
  kbl %>% 
  kable_paper %>% 
  scroll_box(width = "100%", height = "600px")

```  

Here are the numeric variables in order of their correlation (positive or negative) with the target variable, **logTotalPrice**. The variables with the strongest correlation might be useful predictors of the target variable. The top 10 are shown.  

```{r Q1 a2, echo = FALSE, warning=FALSE, message=FALSE}

# numeric vars EDA relative to target variable
numeric_var_stats <- SmartEDA::ExpNumStat(data, gp = "logTotalValue")

# categorical vars EDA relative to target variable
categorical_var_stats <- SmartEDA::ExpCatStat(data, Target = "logTotalValue")

# show sorted table of absolute value of correlation
numeric_var_stats %>% 
  dplyr::select('Vname', 'cor') %>% 
  rename('variable' = 'Vname', 'cor_with_target_var' = 'cor') %>%
  filter(variable != "logTotalValue" & variable != "TotalValue") %>% 
  arrange(desc(abs(cor_with_target_var))) %>%
  slice_head(n = 10) %>% 
  kable() %>% 
  kable_styling()

```  
## b.

We create training and test sets, allocating 30% of the data to training. To make the model training formulas a bit more convenient, we can drop **ID** and **TotalValue** from the training set, since they won't be used for predicting **logTotalValue**.  
```{r Q1 b, echo = FALSE, warning=FALSE, message=FALSE}

# set seed and create index for training + test sets (30% of data for training set)
set.seed(2021)
index <- createDataPartition(data$logTotalValue, times = 1, p = 0.3, list = FALSE)

# create training and test sets
train <- slice(data, index)
test <- slice(data, -index)

# for the training set, let's just drop the ID and TotalValue columns
# so we don't have to write the exclusion in every formula
train <- train %>% dplyr::select(-c('ID', 'TotalValue'))

```  
## c.

We create a linear model to predict **logTotalValue** and use 10-fold cross-validation.  
```{r Q1 c, echo = FALSE, warning=FALSE, message=FALSE}

# OLS model

train_control <- trainControl(method = "cv", # cross-validation
                              number = "10", # ten-fold CV
                              allowParallel = TRUE) # to speed up CV training

# OLS model with all features
ols_1 <- train(logTotalValue ~ .,
               method = "lm",
               data = train,
               trainControl = train_control)

# # simple OLS model with top 3 features based on correlation with target variable
# # (this model is garbage, but I just wanted to try it out of curiousity)
# ols_simple <- train(logTotalValue ~ BldgArea + NumFloors + BuiltFAR,
#                method = "lm",
#                data = train,
#                trainControl = train_control)

```  
## d.

We use penalized linear models for the same task. These models penalize complexity in different ways, but all may reduce the number of coefficients. The best models (ridge / elastic net) improve on the simple linear model a little bit.
```{r Q1 d1, echo = FALSE, warning=FALSE, message=FALSE}

# penalized models

# ridge model
ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = seq(0.05, 0.5, by = 0.025)
)

set.seed(2021)
ridge_1 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = ridge_tune_grid,
  trainControl = train_control)

# LASSO model

tenpowers <- 10^seq(-1, -5, by = -1)

lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = c(tenpowers, tenpowers / 2) 
)

set.seed(2021)
lasso_1 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = lasso_tune_grid,
  trainControl = train_control
)


# elastic net model

enet_tune_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = union(lasso_tune_grid[["lambda"]], ridge_tune_grid[["lambda"]])
)

set.seed(2021)
enet_1 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = enet_tune_grid,
  trainControl = train_control
)

```  
  
Here are the results:  
```{r Q1 d2, echo = FALSE, message = FALSE, warning = FALSE}

# get the number of non-zero coefficients for each model
ols_1_nz_coef <- length(coef(ols_1$finalModel)) # just the length of the list of coefficients
ridge_1_nz_coef <- sum(coef(ridge_1$finalModel, s = ridge_1$bestTune$lambda) > 0) # count coeffs > 0
lasso_1_nz_coef <- sum(coef(lasso_1$finalModel, s = lasso_1$bestTune$lambda) > 0) # count coeffs > 0
enet_1_nz_coef <- sum(coef(enet_1$finalModel, s = enet_1$bestTune$lambda) > 0) # count coeffs > 0

# dataframe of RMSE in cross-validation
cv_rmse_df_one_se <-
  data.frame('Model' = c('OLS', 'Ridge', 'LASSO', 'Elastic Net'), 
             'RMSE' = c(min(ols_1$results$RMSE), min(ridge_1$results$RMSE),
             min(lasso_1$results$RMSE), min(enet_1$results$RMSE)),
             'Non-zero coefficients' = c(ols_1_nz_coef, ridge_1_nz_coef, lasso_1_nz_coef, enet_1_nz_coef))

# show the CV RMSE DF
cv_rmse_df_one_se %>% 
  kable(digits = 3) %>% 
  kable_styling()

```  
## e.

We experiment with setting **oneSE** as the **selection function** in caret's **trainControl**. This follows the "one standard error" rule of picking the simplest model which is within one standard error of the optimal model found through experimentation. You can read more about this in caret's documentation: [link to pdf](https://cran.r-project.org/web/packages/caret/caret.pdf)  
```{r Q1 e, echo = FALSE, warning=FALSE, message=FALSE}

# new train_control with one_se selection function
train_control_one_se <- trainControl(method = "cv", # cross-validation
                                     number = "10", # ten-fold CV
                                     selectionFunction = "onese", # for selecting best model
                                     allowParallel = TRUE) # to speed up CV training


# OLS model with all features
ols_2 <- train(logTotalValue ~ .,
               method = "lm",
               data = train,
               trainControl = train_control_one_se)


# penalized models

# ridge model
set.seed(2021)
ridge_2 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = ridge_tune_grid,
  trainControl = train_control_one_se)

# LASSO model
set.seed(2021)
lasso_2 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = lasso_tune_grid,
  trainControl = train_control_one_se
)


# elastic net model
set.seed(2021)
enet_2 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = enet_tune_grid,
  trainControl = train_control_one_se
)

```  

When we view the results for the **oneSE** approach, we notice that the simple linear model has a slightly higher RMSE, but the performance of the penalized models hasn't changed (at least within three significant digits). Theoretically, the **oneSE** approach should give us a slightly simpler, slightly less well-fit model. (The idea is that this slightly simpler model is less likely to be overfit and should actually perform better in external data.)  

The LASSO model is the simplest, in terms of number of coefficients, and it's performance is certainly "good enough": it only barely trails behind the ridge and elastic net models.  
```{r Q1 e2, echo = FALSE, warning = FALSE, message = FALSE}

# number of non-zero coefficients for each model
ols_2_nz_coef <- length(coef(ols_2$finalModel)) # just the length of the list of coefficients
ridge_2_nz_coef <- sum(coef(ridge_2$finalModel, s = ridge_2$bestTune$lambda) > 0) # count coeffs > 0
lasso_2_nz_coef <- sum(coef(lasso_2$finalModel, s = lasso_2$bestTune$lambda) > 0) # count coeffs > 0
enet_2_nz_coef <- sum(coef(enet_2$finalModel, s = enet_2$bestTune$lambda) > 0) # count coeffs > 0

# dataframe of RMSE in cross-validation
cv_rmse_df_one_se <-
  data.frame('Model' = c('OLS', 'Ridge', 'LASSO', 'Elastic Net'), 
             'RMSE' = c(min(ols_2$results$RMSE), min(ridge_2$results$RMSE),
             min(lasso_2$results$RMSE), min(enet_2$results$RMSE)),
             'Non-zero coefficients' = c(ols_2_nz_coef, ridge_2_nz_coef, lasso_2_nz_coef, enet_2_nz_coef))

# show the CV RMSE DF
cv_rmse_df_one_se %>% 
  kable(digits = 3) %>% 
  kable_styling()
```  
## f.

We try to improve the linear model by using PCA for dimensionality reduction. This improves the fit of the linear model, and it makes the linear model the best performer in terms of RMSE. Results are shown in the PCA models table in the next section.  
```{r Q1 f, echo = FALSE, warning=FALSE, message=FALSE}

# PCA linear model

tune_grid <- data.frame(ncomp = 60:90)

set.seed(2021)

ols_3 <- train(
  logTotalValue ~ .,
  data = train,
  method = "pcr",
  trainControl = train_control,
  tuneGrid = tune_grid,
  preProcess = c("center", "scale")
)

```  
## g.

We apply PCA in the preprocessing before fitting the penalized models. We also drop zero-variance features in pre-processing.  

It does not help to achieve a better fit; RMSE is now **higher** for the penalized models. My intuition is that PCA and penalized models take different approaches to reducing noise in the features. If you run PCA before fitting a penalized model, you don't give the LASSO/ridge/E-net algorithm a chance to punish large coefficients independently - now those features which would normally result in large coefficients exist as linear combinations with other features. Then the linear combinations of features could get punished in a way that is not optimal.  

Generally, we should probably try both PCA and penalized models, and just see which one works better for our problem.  
```{r Q1 g1, echo = FALSE, warning=FALSE, message=FALSE}

# penalized models with PCA in the preProcess
# also drop zero-variance features with "nzv"

# ridge model

set.seed(2021)
ridge_3 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale", "nzv", "pca"),
  tuneGrid = ridge_tune_grid,
  trainControl = train_control)

# LASSO model

set.seed(2021)
lasso_3 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale", "nzv", "pca"),
  tuneGrid = lasso_tune_grid,
  trainControl = train_control
)


# elastic net model

set.seed(2021)
enet_3 <- train(
  logTotalValue ~ .,
  data = train,
  method = "glmnet",
  preProcess = c("center", "scale", "nzv", "pca"),
  tuneGrid = enet_tune_grid,
  trainControl = train_control
)


```  
  
Results for PCA models are shown below:   

```{r, echo = FALSE, warning=FALSE, message=FALSE}


# PCA CV RMSE dataframe with number of components
cv_pca_rmse_df <-
  data.frame('Model' = c('OLS with PCA', 'Ridge with PCA',
                         'LASSO with PCA', 'Elastic Net with PCA'), 
             'RMSE' = c(min(ols_3$results$RMSE), min(ridge_3$results$RMSE),
             min(lasso_3$results$RMSE), min(enet_3$results$RMSE)),
             'Number.of.components' = c(ols_3$bestTune$ncomp, ridge_3$preProcess$numComp, lasso_3$preProcess$numComp, enet_3$preProcess$numComp))

# show the CV RMSE for models with PCA DF
cv_pca_rmse_df %>% 
  kable(digits = 3) %>% 
  kable_styling()

```  

## h.

Based on RMSE in cross-validation, the **best model is the linear model with PCA**. The RMSE in cross-validation is `r ols_3$results$RMSE`. We try it on the test set.  
```{r, echo = FALSE, warning=FALSE, message=FALSE}

# make predictions on the test set and save to vector
ols_3_pred <- predict(ols_3, test)

# get test set RMSE
ols_3_rmse <- RMSE(ols_3_pred, test$logTotalValue)
```  
  
On the test set, the RMSE is a higher: `r ols_3_rmse`  
  
Perhaps we could improve model performance with a larger training set.  

# 2: Clustering with USArrests dataset

## a.  First, we take a quick look at the summary statistics and histograms of the variables.  
```{r Q2 a1, echo = FALSE, warning=FALSE, message=FALSE}

# load data
df <- USArrests

# view basic summary
skim(df) %>% 
  kbl %>% 
  kable_paper %>% 
  scroll_box(width = "100%", height = "600px")

# view histograms
df %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()

```
To prepare the data for clustering, we'll de-mean and scale it. (Basically, standardization.) This should make visualizing the data easier, and it will allow us to compare the variables easily on a common scale.  
```{r Q2 a2, echo = FALSE, warning=FALSE, message=FALSE}

# de-mean the data for easier visualizations and comparisons
df_demeaned <- mutate(df, across(everything(), ~ .x - mean(.x)))

# scale the data to avoid overweighting variables with large values (for example "assault" VS "murder")
df_demeaned_scaled <-  mutate(df_demeaned, across(everything(), ~ .x / sd(.x)))

```  
## b.

Visualize the optimal number of clusters for the demeaned data. We'll probably consider the optimal number to be 2, based on the conspicuous "elbow" in this chart where the sum of squares drops off considerably.  
```{r Q2 b1, echo = FALSE, warning=FALSE, message=FALSE}

# let's see if we can visualize the "elbow" point and spot the optimal number of clusters
fviz_nbclust(df_demeaned_scaled, kmeans, method = "wss")

# looks like we might end up choosing something in the range of 2 to 4
```  
  
We can also lean on NbClust to determine the optimal number of clusters. This method also suggests using 2 clusters.  
```{r Q2 b2, echo = FALSE, warning=FALSE, message=FALSE}

nb <- NbClust(df_demeaned_scaled, method = "kmeans", min.nc = 2, max.nc = 10, index = "all")

# NbClust offers us the following conclusion:
# * According to the majority rule, the best number of clusters is  2 
```  
## c.

Here, we use k-means clustering and plot observations colored by clusters. On the X axis is **Murder** and  on the Y axis is **Urban Population**.  
```{r Q2 c, echo = FALSE, warning=FALSE, message=FALSE}

set.seed(2021)

km <- kmeans(df_demeaned_scaled, centers = 2) #set centers = 2 ; that's the number of clusters we want

df_demeaned_scaled_w_clusters <- mutate(df_demeaned_scaled, cluster = factor(km$cluster))

ggplot(df_demeaned_scaled_w_clusters, aes(x = Murder, y = UrbanPop, color = cluster)) +
  geom_point()

```  
## d.

We determine the first two principal components and plot them together below. There appear to be two distinct groups. I have added a blue annotation line to mark them clearly.  
```{r Q2 d1, echo = FALSE, warning=FALSE, message=FALSE}
# get principal components
pca_result <- prcomp(df, scale = TRUE)

# get just the first 2 PC's
first_two_pc <- as_tibble(pca_result$x[, 1:2])

# show PC1 and PC2 with annotation
ggplot(first_two_pc, aes(x = PC1, y = PC2)) +
  geom_point() +
  annotate("segment", x = -0.6, xend = -0.2, y = -1.5, yend = 2,
  colour = "blue") +
  annotate("text", x = c(-2,1), y = c(2,2), label = c("Group A", "Group B"), color = "red", size = 5)
```  


If we overlay the clusters we identified above (before PCA), we observe a similar pair of grouped data points. They have a similar size and location, and the division between the clusters occurs right along the edge of Group B in the principal components.  

```{r Q2 d2, echo = FALSE, warning=FALSE, message=FALSE}

# PCs and clustered data shown together
ggplot(first_two_pc, aes(x = PC1, y = PC2)) +
  geom_point() +
  geom_point(data = df_demeaned_scaled_w_clusters, aes(x = Murder, y = UrbanPop, color = cluster)) +
  labs(x = "PC1 / Murder", y = "PC2 / UrbanPop")+
  annotate("text", x = c(-2,1), y = c(2,2), label = c("Group A", "Group B"), color = "red", size = 5)


```